---
phase: 01-ml-pipeline-testing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/tests/unit/test_build_matrix.py
  - backend/tests/unit/test_feedback_mapping.py
  - backend/tests/unit/test_recommender_class.py
autonomous: true
---

<objective>
Add unit tests for all untested backend modules: build_matrix logic, feedback_mapping (consolidate from src/), and Recommender class internal methods.

Purpose: Close the three largest unit test gaps — build_matrix has zero tests, feedback_mapping tests are misplaced, and the Recommender class internals are only tested indirectly via integration tests.
Output: Three new unit test files with comprehensive coverage of all untested pure functions and class methods.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Source files to test:
@backend/src/movie_recommender/services/recommender/learning/build_matrix.py
@backend/src/movie_recommender/services/recommender/serving/feedback_mapping.py
@backend/src/movie_recommender/services/recommender/main.py

Existing test patterns:
@backend/tests/conftest.py
@backend/tests/unit/test_online_updater.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create unit tests for build_matrix.py</name>
  <files>backend/tests/unit/test_build_matrix.py</files>
  <action>
Create unit tests for the build_matrix module. Focus on the testable pure logic:

1. **TestToNative class** — test the `_to_native()` helper (it's a nested function, so test it by importing `build_sparse_matrix` and testing the matrix output indirectly, OR extract and test the conversion logic):
   - numpy int keys → Python int keys
   - numpy int values → Python int values
   - Mixed key types handled

2. **TestBuildSparseMatrix class** — test the sparse matrix construction logic by creating a small in-memory DataFrame and verifying the output. Use `unittest.mock.patch` to redirect all file paths (TRAIN_PATH, MATRIX_PATH, MAPPINGS_PATH, ARTIFACTS) to a tmp_path, same pattern as test_full_pipeline.py:
   - test_matrix_shape_matches_unique_users_and_movies: Create df with 3 users × 4 movies, verify R_train.shape == (3, 4)
   - test_matrix_nonzero_count_matches_interactions: Verify R_train.nnz equals number of rows in df
   - test_mappings_are_bijective: Verify movie_id_to_index and index_to_movie_id are inverses
   - test_user_mappings_cover_all_users: Every user_id in df appears in user_id_to_index
   - test_matrix_values_are_preferences: Verify specific cells contain the correct preference values
   - test_artifacts_saved_to_disk: After build_sparse_matrix(), verify R_train.npz and mappings.json exist

Use a helper to create a minimal DataFrame:
```python
def _make_train_df(rows):
    return pd.DataFrame(rows, columns=["user_id", "movie_id", "preference"])
```

Patch these module-level constants:
- `movie_recommender.services.recommender.learning.build_matrix.TRAIN_PATH`
- `movie_recommender.services.recommender.learning.build_matrix.MATRIX_PATH`
- `movie_recommender.services.recommender.learning.build_matrix.MAPPINGS_PATH`
- `movie_recommender.services.recommender.learning.build_matrix.ARTIFACTS`
  </action>
  <verify>cd backend && .venv/bin/python -m pytest tests/unit/test_build_matrix.py -v</verify>
  <done>All build_matrix tests pass. Matrix shape, nnz, mapping bijectivity, preference values, and artifact file creation are validated.</done>
</task>

<task type="auto">
  <name>Task 2: Consolidate feedback_mapping tests and add edge cases</name>
  <files>backend/tests/unit/test_feedback_mapping.py</files>
  <action>
Create backend/tests/unit/test_feedback_mapping.py that consolidates the tests from src/movie_recommender/services/recommender/tests/test_feedback_mapping.py into the proper test directory, and adds missing edge cases.

**Existing tests to bring over (from src/):**
- test_dislike_maps_to_negative_one
- test_supercharged_dislike_maps_to_negative_two
- test_like_maps_to_positive_one
- test_supercharged_like_maps_to_positive_two
- test_skip_maps_to_zero

**New edge case tests to add:**
- test_skip_supercharged_still_zero: SwipeAction.SKIP with is_supercharged=True should still return 0 (skip ignores supercharge)
- test_invalid_action_raises_value_error: Pass an invalid string to verify ValueError is raised (the function has a `raise ValueError` at the bottom)
- test_all_actions_return_int: Verify return type is int for all valid combinations

Use the SwipeAction enum from movie_recommender.schemas.interactions.

Organize into classes:
- TestLikePreference
- TestDislikePreference
- TestSkipPreference
- TestEdgeCases
  </action>
  <verify>cd backend && .venv/bin/python -m pytest tests/unit/test_feedback_mapping.py -v</verify>
  <done>All feedback_mapping tests pass including new edge cases. Tests properly imported from schemas and feedback_mapping modules.</done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for Recommender class internals</name>
  <files>backend/tests/unit/test_recommender_class.py</files>
  <action>
Create unit tests for the Recommender class methods that are currently only tested indirectly through integration tests. Use the `synthetic_artifacts` and `recommender` fixtures from conftest.py.

**TestRequireArtifacts class:**
- test_raises_when_no_artifacts: Create Recommender with artifacts=None, call _require_artifacts(), assert RuntimeError with message containing "not available"
- test_returns_artifacts_when_loaded: Use recommender fixture, call _require_artifacts(), assert returns the artifacts object
- test_error_message_includes_load_details: Set _artifact_load_error to "file missing", assert RuntimeError message includes "file missing"

**TestColdStartVector class:**
- test_cold_start_is_mean_of_all_users: With synthetic_artifacts (3 users), verify _cold_start_vector returns mean of user_embeddings along axis=0
- test_cold_start_dtype_is_float32: Verify output dtype
- test_cold_start_shape_matches_embedding_dim: Verify shape is (4,) matching dim=4

**TestBaseUserVector class:**
- test_known_user_returns_their_embedding: user_id="1" (maps to index 0) should return user_embeddings[0]
- test_unknown_user_returns_cold_start: user_id="99999" should return mean vector
- test_non_numeric_user_returns_cold_start: user_id="abc" should return mean vector
- test_none_user_id_returns_cold_start: user_id=None should return cold start (but note: _to_int_user_id handles None)

**TestCurrentUserVector class:**
- test_returns_base_when_no_online_vector: Fresh recommender, user_id="1" returns base embedding
- test_returns_online_vector_when_present: Set recommender.online_user_vectors["1"] = custom_vec, verify it returns custom_vec
- test_online_vector_takes_precedence: Even for known user, if online vector exists it takes precedence

**TestGetTopN class:**
- test_returns_correct_count: get_top_n("1", n=3) returns exactly 3 tuples
- test_returns_tuples_of_int_and_str: Each result is (int, str) - movie_id and title
- test_all_movies_seen_returns_empty: Mark all 5 movies as seen, get_top_n returns []
- test_n_larger_than_available_returns_all_unseen: With 5 movies, 3 seen, n=10 returns 2
- test_scores_use_dot_product: For action fan (user 1), action movies should rank highest (scores for movie 100 and 103 are highest)

**TestUpdateUser class:**
- test_unknown_movie_marks_seen_but_no_vector_update: update_user with movie_id=999 (not in artifacts), verify seen set updated but no online vector created
- test_skip_marks_seen_does_not_update_vector: update_user with SKIP, verify movie in seen set, verify vector unchanged
- test_like_creates_online_vector: update_user with LIKE, verify online_user_vectors has entry
- test_multiple_updates_accumulate: Two likes on different movies, verify seen set has 2 entries

Use `Recommender.__new__(Recommender)` pattern from conftest.py to bypass __init__ disk I/O.
  </action>
  <verify>cd backend && .venv/bin/python -m pytest tests/unit/test_recommender_class.py -v</verify>
  <done>All Recommender class unit tests pass. _require_artifacts, _cold_start_vector, _base_user_vector, _current_user_vector, get_top_n, and update_user are all tested directly.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `cd backend && .venv/bin/python -m pytest tests/unit/ -v` — all unit tests pass
- [ ] `cd backend && .venv/bin/python -m pytest tests/ -v` — full test suite still passes (no regressions)
- [ ] New test count is at least 30 more than the original 122
</verification>

<success_criteria>
- All three new test files created and passing
- build_matrix has tests for matrix construction, mapping bijectivity, artifact persistence
- feedback_mapping tests consolidated from src/ to tests/unit/ with new edge cases
- Recommender class has direct unit tests for all internal methods
- No regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/01-ml-pipeline-testing/01-01-SUMMARY.md`
</output>
